{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aae7494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple linear regression\n",
    "# Import dataset and encode categorical variables, fit multiple linear regression\n",
    "# find best model using backward elimination\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('50_Startups.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 4].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e95d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Encode the categorical variable (State) using LabelEncoder\n",
    "label_encoder_X = LabelEncoder()\n",
    "X[:, 3] = label_encoder_X.fit_transform(X[:, 3])\n",
    "\n",
    "# Create a ColumnTransformer to one-hot encode the categorical variable\n",
    "# Specify the column to be transformed (State) and the OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\n",
    "\n",
    "# Apply the ColumnTransformer to the dataset\n",
    "X = ct.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a17664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avoiding the Dummy variable trap \n",
    "#without taking all dummy variables but - 1\n",
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "817095a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b36dc618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Multiple linear regression to the training set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "#Predicting test set results\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b84f2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting statsmodels\n",
      "  Downloading statsmodels-0.14.0-cp310-cp310-macosx_11_0_arm64.whl (9.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /Users/abhinavsharma/anaconda3/lib/python3.10/site-packages (from statsmodels) (1.26.0)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.4 in /Users/abhinavsharma/anaconda3/lib/python3.10/site-packages (from statsmodels) (1.11.3)\n",
      "Requirement already satisfied: pandas>=1.0 in /Users/abhinavsharma/anaconda3/lib/python3.10/site-packages (from statsmodels) (2.1.1)\n",
      "Collecting patsy>=0.5.2 (from statsmodels)\n",
      "  Downloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.8/233.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=21.3 in /Users/abhinavsharma/anaconda3/lib/python3.10/site-packages (from statsmodels) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/abhinavsharma/anaconda3/lib/python3.10/site-packages (from pandas>=1.0->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/abhinavsharma/anaconda3/lib/python3.10/site-packages (from pandas>=1.0->statsmodels) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/abhinavsharma/anaconda3/lib/python3.10/site-packages (from pandas>=1.0->statsmodels) (2023.3)\n",
      "Requirement already satisfied: six in /Users/abhinavsharma/anaconda3/lib/python3.10/site-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n",
      "Installing collected packages: patsy, statsmodels\n",
      "Successfully installed patsy-0.5.3 statsmodels-0.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72826f45",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'statsmodels.formula.api' has no attribute 'OLS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#fit model with all predictors\u001b[39;00m\n\u001b[1;32m      6\u001b[0m X_opt \u001b[38;5;241m=\u001b[39m X[:,[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m]]\n\u001b[0;32m----> 7\u001b[0m regressor_OLS \u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOLS\u001b[49m(endog \u001b[38;5;241m=\u001b[39m y, exog \u001b[38;5;241m=\u001b[39m X_opt)\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m      8\u001b[0m regressor_OLS\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#let's remove the variable with the highest  pvalue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'statsmodels.formula.api' has no attribute 'OLS'"
     ]
    }
   ],
   "source": [
    "#build optimal model using Backward Elimination(btw its not working due to some reason)\n",
    "import statsmodels.formula.api as sm\n",
    "#add constant column with all 1 to have a b0\n",
    "X = np.append(arr = np.ones((50,1)).astype(int), values = X, axis = 1)\n",
    "#fit model with all predictors\n",
    "X_opt = X[:,[0,1,2,3,4,5]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "regressor_OLS.summary()\n",
    "#let's remove the variable with the highest  pvalue\n",
    "X_opt = X[:,[0,1,3,4,5]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "regressor_OLS.summary()\n",
    "#let's remove the variable with the highest  pvalue\n",
    "X_opt = X[:,[0,3,4,5]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "regressor_OLS.summary()\n",
    "#let's remove the variable with the highest  pvalue\n",
    "X_opt = X[:,[0,3,5]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "regressor_OLS.summary()\n",
    "#let's remove the variable with the highest  pvalue\n",
    "X_opt = X[:,[0,3,5]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9aa1dbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinavsharma/anaconda3/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/abhinavsharma/anaconda3/lib/python3.10/site-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 5 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n",
      "/Users/abhinavsharma/anaconda3/lib/python3.10/site-packages/statsmodels/regression/linear_model.py:1794: RuntimeWarning: divide by zero encountered in divide\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "/Users/abhinavsharma/anaconda3/lib/python3.10/site-packages/statsmodels/regression/linear_model.py:1794: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "/Users/abhinavsharma/anaconda3/lib/python3.10/site-packages/statsmodels/regression/linear_model.py:1716: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Profit</td>      <th>  R-squared:         </th> <td>   1.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>     nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>     nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 01 Oct 2023</td> <th>  Prob (F-statistic):</th>  <td>   nan</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>00:38:49</td>     <th>  Log-Likelihood:    </th> <td>  44.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>     5</td>      <th>  AIC:               </th> <td>  -78.57</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     0</td>      <th>  BIC:               </th> <td>  -80.52</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td>-4.723e+05</td> <td>      inf</td> <td>       -0</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>R&D Spend</th>       <td>   11.7797</td> <td>      inf</td> <td>        0</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Administration</th>  <td>    2.9556</td> <td>      inf</td> <td>        0</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Marketing Spend</th> <td>   -3.2919</td> <td>      inf</td> <td>       -0</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>State_0</th>         <td>-2.374e+05</td> <td>      inf</td> <td>       -0</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>State_1</th>         <td>-1.002e+05</td> <td>      inf</td> <td>       -0</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>State_2</th>         <td>-1.347e+05</td> <td>      inf</td> <td>       -0</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>   nan</td> <th>  Durbin-Watson:     </th> <td>   0.551</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td>   nan</td> <th>  Jarque-Bera (JB):  </th> <td>   1.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.249</td> <th>  Prob(JB):          </th> <td>   0.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.919</td> <th>  Cond. No.          </th> <td>5.09e+07</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The input rank is higher than the number of observations.<br/>[3] The condition number is large, 5.09e+07. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      Profit      & \\textbf{  R-squared:         } &     1.000   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &       nan   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &       nan   \\\\\n",
       "\\textbf{Date:}             & Sun, 01 Oct 2023 & \\textbf{  Prob (F-statistic):} &      nan    \\\\\n",
       "\\textbf{Time:}             &     00:38:49     & \\textbf{  Log-Likelihood:    } &    44.286   \\\\\n",
       "\\textbf{No. Observations:} &           5      & \\textbf{  AIC:               } &    -78.57   \\\\\n",
       "\\textbf{Df Residuals:}     &           0      & \\textbf{  BIC:               } &    -80.52   \\\\\n",
       "\\textbf{Df Model:}         &           4      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                         & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}           &   -4.723e+05  &          inf     &        -0  &           nan        &          nan    &          nan     \\\\\n",
       "\\textbf{R\\&D Spend}      &      11.7797  &          inf     &         0  &           nan        &          nan    &          nan     \\\\\n",
       "\\textbf{Administration}  &       2.9556  &          inf     &         0  &           nan        &          nan    &          nan     \\\\\n",
       "\\textbf{Marketing Spend} &      -3.2919  &          inf     &        -0  &           nan        &          nan    &          nan     \\\\\n",
       "\\textbf{State\\_0}        &   -2.374e+05  &          inf     &        -0  &           nan        &          nan    &          nan     \\\\\n",
       "\\textbf{State\\_1}        &   -1.002e+05  &          inf     &        -0  &           nan        &          nan    &          nan     \\\\\n",
       "\\textbf{State\\_2}        &   -1.347e+05  &          inf     &        -0  &           nan        &          nan    &          nan     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &    nan & \\textbf{  Durbin-Watson:     } &    0.551  \\\\\n",
       "\\textbf{Prob(Omnibus):} &    nan & \\textbf{  Jarque-Bera (JB):  } &    1.301  \\\\\n",
       "\\textbf{Skew:}          &  1.249 & \\textbf{  Prob(JB):          } &    0.522  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.919 & \\textbf{  Cond. No.          } & 5.09e+07  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The input rank is higher than the number of observations. \\newline\n",
       " [3] The condition number is large, 5.09e+07. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 Profit   R-squared:                       1.000\n",
       "Model:                            OLS   Adj. R-squared:                    nan\n",
       "Method:                 Least Squares   F-statistic:                       nan\n",
       "Date:                Sun, 01 Oct 2023   Prob (F-statistic):                nan\n",
       "Time:                        00:38:49   Log-Likelihood:                 44.286\n",
       "No. Observations:                   5   AIC:                            -78.57\n",
       "Df Residuals:                       0   BIC:                            -80.52\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const           -4.723e+05        inf         -0        nan         nan         nan\n",
       "R&D Spend          11.7797        inf          0        nan         nan         nan\n",
       "Administration      2.9556        inf          0        nan         nan         nan\n",
       "Marketing Spend    -3.2919        inf         -0        nan         nan         nan\n",
       "State_0         -2.374e+05        inf         -0        nan         nan         nan\n",
       "State_1         -1.002e+05        inf         -0        nan         nan         nan\n",
       "State_2         -1.347e+05        inf         -0        nan         nan         nan\n",
       "==============================================================================\n",
       "Omnibus:                          nan   Durbin-Watson:                   0.551\n",
       "Prob(Omnibus):                    nan   Jarque-Bera (JB):                1.301\n",
       "Skew:                           1.249   Prob(JB):                        0.522\n",
       "Kurtosis:                       2.919   Cond. No.                     5.09e+07\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The input rank is higher than the number of observations.\n",
       "[3] The condition number is large, 5.09e+07. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a hypothetical dataset\n",
    "data = {\n",
    "    'R&D Spend': [165349, 162598, 153442, 144372, 142107],\n",
    "    'Administration': [136897, 151377, 101145, 118671, 91391],\n",
    "    'Marketing Spend': [471784, 443899, 407934, 383199, 366168],\n",
    "    'State': ['New York', 'California', 'Florida', 'New York', 'Florida'],\n",
    "    'Profit': [192261, 191792, 191050, 182901, 166187]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Encode categorical values\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['State'] = label_encoder.fit_transform(df['State'])\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "state_encoded = one_hot_encoder.fit_transform(df[['State']])\n",
    "state_columns = ['State_' + str(i) for i in range(state_encoded.shape[1])]\n",
    "state_df = pd.DataFrame(state_encoded, columns=state_columns)\n",
    "df = pd.concat([df, state_df], axis=1)\n",
    "df = df.drop('State', axis=1)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop('Profit', axis=1)\n",
    "y = df['Profit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Fitting Multiple linear regression to the training set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predicting test set results\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Build optimal model using Backward Elimination\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Add constant column with all 1's to have a b0\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit model with all predictors\n",
    "regressor_OLS = sm.OLS(y, X).fit()\n",
    "regressor_OLS.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
